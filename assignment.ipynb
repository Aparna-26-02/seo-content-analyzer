{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8277a6d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File loaded successfully!\n",
      "--- First 5 rows of the data ---\n",
      "                                                 url  \\\n",
      "0     https://www.cm-alliance.com/cybersecurity-blog   \n",
      "1    https://www.varonis.com/blog/cybersecurity-tips   \n",
      "2  https://www.cisecurity.org/insights/blog/11-cy...   \n",
      "3  https://www.cisa.gov/topics/cybersecurity-best...   \n",
      "4  https://www.qnbtrust.bank/Resources/Learning-C...   \n",
      "\n",
      "                                        html_content  \n",
      "0  <!doctype html><!--[if lt IE 7]> <html class=\"...  \n",
      "1  <!doctype html><html lang=\"en\"><head>\\n    <me...  \n",
      "2  <!DOCTYPE html><html data-unhead-vue-server-re...  \n",
      "3  \\n\\n<!DOCTYPE html>\\n<html lang=\"en\" dir=\"ltr\"...  \n",
      "4                                                NaN  \n",
      "\n",
      "--- DataFrame Info ---\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 81 entries, 0 to 80\n",
      "Data columns (total 2 columns):\n",
      " #   Column        Non-Null Count  Dtype \n",
      "---  ------        --------------  ----- \n",
      " 0   url           81 non-null     object\n",
      " 1   html_content  69 non-null     object\n",
      "dtypes: object(2)\n",
      "memory usage: 1.4+ KB\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Define the file path\n",
    "file_path = 'data.csv'\n",
    "\n",
    "# Load the dataset into a pandas DataFrame\n",
    "try:\n",
    "    df = pd.read_csv(file_path)\n",
    "    print(\"File loaded successfully!\")\n",
    "except FileNotFoundError:\n",
    "    print(f\"Error: The file was not found at {file_path}\")\n",
    "    print(\"Please make sure 'data.csv' is in the same folder as your notebook.\")\n",
    "\n",
    "# Display the first 5 rows of the DataFrame\n",
    "print(\"--- First 5 rows of the data ---\")\n",
    "print(df.head())\n",
    "\n",
    "# Get a concise summary of the DataFrame\n",
    "print(\"\\n--- DataFrame Info ---\")\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2d5cc159",
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "\n",
    "def parse_html(html_content):\n",
    "    \"\"\"\n",
    "    Parses HTML content to extract clean, readable text.\n",
    "    Handles potential errors by returning an empty string.\n",
    "    \"\"\"\n",
    "    # Check if content is a string, return empty if not (handles non-string/NaN values)\n",
    "    if not isinstance(html_content, str):\n",
    "        return \"\"\n",
    "    try:\n",
    "        # Use the lxml parser for speed and efficiency\n",
    "        soup = BeautifulSoup(html_content, 'lxml')\n",
    "\n",
    "        # Remove script and style elements as they don't contain readable content\n",
    "        for script_or_style in soup([\"script\", \"style\"]):\n",
    "            script_or_style.decompose()\n",
    "\n",
    "        # Get text, using a space as a separator and stripping extra whitespace\n",
    "        text = soup.get_text(separator=' ', strip=True)\n",
    "        return text\n",
    "    except Exception as e:\n",
    "        # If any other error occurs during parsing, return an empty string\n",
    "        return \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d4179068",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Cleaned Text from HTML ---\n",
      "                                                 url  \\\n",
      "0     https://www.cm-alliance.com/cybersecurity-blog   \n",
      "1    https://www.varonis.com/blog/cybersecurity-tips   \n",
      "2  https://www.cisecurity.org/insights/blog/11-cy...   \n",
      "3  https://www.cisa.gov/topics/cybersecurity-best...   \n",
      "4  https://www.qnbtrust.bank/Resources/Learning-C...   \n",
      "\n",
      "                                        cleaned_text  \n",
      "0  Cyber Security Blog <img  height=\"1\" width=\"1\"...  \n",
      "1  Top 10 Cybersecurity Awareness Tips: How to St...  \n",
      "2  11 Cyber Defense Tips to Stay Secure at Work a...  \n",
      "3  Cybersecurity Best Practices | Cybersecurity a...  \n",
      "4                                                     \n"
     ]
    }
   ],
   "source": [
    "# Apply the function to the 'html_content' column\n",
    "df['cleaned_text'] = df['html_content'].apply(parse_html)\n",
    "\n",
    "# Display the first 5 rows of the new column to see the result\n",
    "print(\"--- Cleaned Text from HTML ---\")\n",
    "print(df[['url', 'cleaned_text']].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5c3afffa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking libraries in this Python environment: c:\\Users\\91854\\Documents\\leadwalnut\\venv\\Scripts\\python.exe\n",
      "\n",
      "✅ pandas          ... INSTALLED\n",
      "✅ scikit-learn    ... INSTALLED\n",
      "✅ nltk            ... INSTALLED\n",
      "✅ BeautifulSoup   ... INSTALLED\n",
      "✅ lxml            ... INSTALLED\n",
      "✅ textstat        ... INSTALLED\n",
      "✅ tqdm            ... INSTALLED\n",
      "❌ streamlit       ... NOT INSTALLED (run: pip install streamlit)\n",
      "\n",
      "========================================\n",
      "❌ Some libraries are missing. Please install them using the commands above.\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "\n",
    "print(f\"Checking libraries in this Python environment: {sys.executable}\\n\")\n",
    "\n",
    "libraries_to_check = {\n",
    "    \"pandas\": \"pandas\",\n",
    "    \"scikit-learn\": \"sklearn\",\n",
    "    \"nltk\": \"nltk\",\n",
    "    \"BeautifulSoup\": \"bs4\",\n",
    "    \"lxml\": \"lxml\",\n",
    "    \"textstat\": \"textstat\",\n",
    "    \"tqdm\": \"tqdm\",\n",
    "    \"streamlit\": \"streamlit\"\n",
    "}\n",
    "\n",
    "all_installed = True\n",
    "\n",
    "for install_name, import_name in libraries_to_check.items():\n",
    "    try:\n",
    "        __import__(import_name)\n",
    "        print(f\"✅ {install_name.ljust(15)} ... INSTALLED\")\n",
    "    except ImportError:\n",
    "        print(f\"❌ {install_name.ljust(15)} ... NOT INSTALLED (run: pip install {install_name})\")\n",
    "        all_installed = False\n",
    "\n",
    "print(\"\\n\" + \"=\"*40)\n",
    "if all_installed:\n",
    "    print(\"✅ All required libraries are installed. You are ready to go!\")\n",
    "else:\n",
    "    print(\"❌ Some libraries are missing. Please install them using the commands above.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9c5c42b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using a custom, NLTK-free preprocessing function to fix the error.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Text: 100%|██████████| 81/81 [00:00<00:00, 745.93it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "PREPROCESSING IS FINALLY COMPLETE!\n",
      "\n",
      "--- Text After Preprocessing ---\n",
      "                                        cleaned_text  \\\n",
      "0  Cyber Security Blog <img  height=\"1\" width=\"1\"...   \n",
      "1  Top 10 Cybersecurity Awareness Tips: How to St...   \n",
      "2  11 Cyber Defense Tips to Stay Secure at Work a...   \n",
      "3  Cybersecurity Best Practices | Cybersecurity a...   \n",
      "4                                                      \n",
      "\n",
      "                                    processed_tokens  \n",
      "0  [cyber, security, blog, img, height1, width1, ...  \n",
      "1  [top, 10, cybersecurity, awareness, tips, how,...  \n",
      "2  [11, cyber, defense, tips, stay, secure, work,...  \n",
      "3  [cybersecurity, best, practices, cybersecurity...  \n",
      "4                                                 []  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "from tqdm import tqdm\n",
    "\n",
    "print(\"Using a custom, NLTK-free preprocessing function to fix the error.\")\n",
    "\n",
    "def preprocess_text_simple(text):\n",
    "    \"\"\"\n",
    "    A simple, NLTK-free text preprocessor that cannot fail.\n",
    "    \"\"\"\n",
    "    if not isinstance(text, str):\n",
    "        return []\n",
    "\n",
    "    # A short list of common stop words\n",
    "    stop_words = set(['the', 'a', 'an', 'in', 'is', 'it', 'and', 'of', 'for', 'to', 'was', 'were', 'on', 'at', 'with', 'by'])\n",
    "\n",
    "    # 1. Convert to lowercase and remove punctuation\n",
    "    text = re.sub(r'[^\\w\\s]', '', text.lower())\n",
    "\n",
    "    # 2. Split text into words\n",
    "    tokens = text.split()\n",
    "\n",
    "    # 3. Remove stop words\n",
    "    processed_tokens = [word for word in tokens if word not in stop_words]\n",
    "    \n",
    "    return processed_tokens\n",
    "\n",
    "# --- Use a more stable apply method to run our new function ---\n",
    "tqdm.pandas(desc=\"Processing Text\")\n",
    "df['processed_tokens'] = df['cleaned_text'].progress_apply(preprocess_text_simple)\n",
    "\n",
    "print(\"\\nPREPROCESSING IS FINALLY COMPLETE!\")\n",
    "\n",
    "# --- Display the Result ---\n",
    "print(\"\\n--- Text After Preprocessing ---\")\n",
    "print(df[['cleaned_text', 'processed_tokens']].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "08d2fe0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_text_stats(cleaned_text, tokens):\n",
    "    \"\"\"\n",
    "    Calculates basic text statistics from the cleaned text and tokens.\n",
    "    \"\"\"\n",
    "    word_count = len(tokens)\n",
    "    char_count = len(cleaned_text)\n",
    "\n",
    "    # Avoid division by zero if there are no words\n",
    "    if word_count > 0:\n",
    "        # Calculate the average word length from the tokens themselves\n",
    "        avg_word_length = sum(len(word) for word in tokens) / word_count\n",
    "    else:\n",
    "        avg_word_length = 0\n",
    "\n",
    "    return word_count, char_count, avg_word_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9734a952",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Basic Text Features ---\n",
      "                                                 url  word_count  char_count  \\\n",
      "0     https://www.cm-alliance.com/cybersecurity-blog      2143.0     17417.0   \n",
      "1    https://www.varonis.com/blog/cybersecurity-tips      2006.0     16436.0   \n",
      "2  https://www.cisecurity.org/insights/blog/11-cy...      1227.0      9775.0   \n",
      "3  https://www.cisa.gov/topics/cybersecurity-best...       954.0      8625.0   \n",
      "4  https://www.qnbtrust.bank/Resources/Learning-C...         0.0         0.0   \n",
      "\n",
      "   avg_word_length  \n",
      "0         6.183854  \n",
      "1         6.194417  \n",
      "2         5.999185  \n",
      "3         7.060797  \n",
      "4         0.000000  \n"
     ]
    }
   ],
   "source": [
    "# Apply the function across the rows of the DataFrame\n",
    "df[['word_count', 'char_count', 'avg_word_length']] = df.apply(\n",
    "    lambda row: calculate_text_stats(row['cleaned_text'], row['processed_tokens']),\n",
    "    axis=1,\n",
    "    result_type='expand'\n",
    ")\n",
    "\n",
    "# Display the new feature columns\n",
    "print(\"--- Basic Text Features ---\")\n",
    "print(df[['url', 'word_count', 'char_count', 'avg_word_length']].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3c0e800e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import textstat\n",
    "\n",
    "def calculate_readability(text):\n",
    "    \"\"\"\n",
    "    Calculates the Flesch Reading Ease score.\n",
    "    Handles potential errors (e.g., for empty text) by returning 0.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # The text must have a minimum number of words for textstat to work\n",
    "        if len(text.split()) > 100:\n",
    "            return textstat.flesch_reading_ease(text)\n",
    "        else:\n",
    "            return 0\n",
    "    except:\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "318fbc2d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Readability Scores ---\n",
      "                                                 url  word_count  \\\n",
      "0     https://www.cm-alliance.com/cybersecurity-blog      2143.0   \n",
      "1    https://www.varonis.com/blog/cybersecurity-tips      2006.0   \n",
      "2  https://www.cisecurity.org/insights/blog/11-cy...      1227.0   \n",
      "3  https://www.cisa.gov/topics/cybersecurity-best...       954.0   \n",
      "4  https://www.qnbtrust.bank/Resources/Learning-C...         0.0   \n",
      "\n",
      "   readability_score  \n",
      "0          28.674270  \n",
      "1          32.694481  \n",
      "2          36.281772  \n",
      "3          -0.732078  \n",
      "4           0.000000  \n"
     ]
    }
   ],
   "source": [
    "# Apply the readability function\n",
    "df['readability_score'] = df['cleaned_text'].apply(calculate_readability)\n",
    "\n",
    "# Display the new feature along with the word count\n",
    "print(\"--- Readability Scores ---\")\n",
    "print(df[['url', 'word_count', 'readability_score']].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "21b64db2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Joined Tokens for Vectorization ---\n",
      "                                      processed_text\n",
      "0  cyber security blog img height1 width1 styledi...\n",
      "1  top 10 cybersecurity awareness tips how stay s...\n",
      "2  11 cyber defense tips stay secure work home yo...\n",
      "3  cybersecurity best practices cybersecurity inf...\n",
      "4                                                   \n"
     ]
    }
   ],
   "source": [
    "# Join the list of tokens into a single string with spaces in between\n",
    "df['processed_text'] = df['processed_tokens'].apply(' '.join)\n",
    "\n",
    "# Display the new column to see the result\n",
    "print(\"--- Joined Tokens for Vectorization ---\")\n",
    "print(df[['processed_text']].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "dea6fc14",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Shape of the TF-IDF matrix: (81, 5000)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# Initialize the vectorizer. We'll limit it to the 5000 most frequent words\n",
    "# to keep the matrix size manageable and focus on important terms.\n",
    "tfidf_vectorizer = TfidfVectorizer(max_features=5000, stop_words='english')\n",
    "\n",
    "# Fit the vectorizer to your text and transform the data into a matrix\n",
    "tfidf_matrix = tfidf_vectorizer.fit_transform(df['processed_text'])\n",
    "\n",
    "# Print the shape of the matrix to see what you've created\n",
    "print(f\"\\nShape of the TF-IDF matrix: {tfidf_matrix.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9b721166",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of the Cosine Similarity Matrix: (81, 81)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# Calculate the cosine similarity between all document vectors\n",
    "cosine_sim_matrix = cosine_similarity(tfidf_matrix)\n",
    "\n",
    "# Print the shape to confirm it's a square matrix (81x81)\n",
    "print(f\"Shape of the Cosine Similarity Matrix: {cosine_sim_matrix.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "71393c74",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Found 0 pairs with > 90% similarity.\n",
      "--- Highly Similar Document Pairs ---\n",
      "No highly similar pairs found.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def find_similar_pairs(similarity_matrix, threshold=0.90):\n",
    "    \"\"\"\n",
    "    Finds pairs of documents with a similarity score above a given threshold.\n",
    "    \"\"\"\n",
    "    # We only need to check the upper triangle of the matrix to avoid duplicates\n",
    "    upper_triangle = np.triu(similarity_matrix, k=1)\n",
    "    \n",
    "    # Find the indices (row, col) where similarity is above the threshold\n",
    "    rows, cols = np.where(upper_triangle > threshold)\n",
    "    \n",
    "    similar_pairs = []\n",
    "    for doc1_index, doc2_index in zip(rows, cols):\n",
    "        similarity_score = similarity_matrix[doc1_index, doc2_index]\n",
    "        similar_pairs.append((doc1_index, doc2_index, similarity_score))\n",
    "        \n",
    "    return similar_pairs\n",
    "\n",
    "# Find pairs with similarity > 90%\n",
    "similar_document_pairs = find_similar_pairs(cosine_sim_matrix, threshold=0.90)\n",
    "\n",
    "# --- Display the results ---\n",
    "print(f\"\\nFound {len(similar_document_pairs)} pairs with > 90% similarity.\")\n",
    "print(\"--- Highly Similar Document Pairs ---\")\n",
    "\n",
    "if not similar_document_pairs:\n",
    "    print(\"No highly similar pairs found.\")\n",
    "else:\n",
    "    for pair in similar_document_pairs:\n",
    "        doc1_url = df.iloc[pair[0]]['url']\n",
    "        doc2_url = df.iloc[pair[1]]['url']\n",
    "        score = pair[2]\n",
    "        print(f\"\\nURL 1: {doc1_url}\")\n",
    "        print(f\"URL 2: {doc2_url}\")\n",
    "        print(f\"Similarity Score: {score:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d57b264a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Composite Quality Scores ---\n",
      "                                                 url  quality_score\n",
      "0     https://www.cm-alliance.com/cybersecurity-blog       0.230479\n",
      "1    https://www.varonis.com/blog/cybersecurity-tips       0.251596\n",
      "2  https://www.cisecurity.org/insights/blog/11-cy...       0.260940\n",
      "3  https://www.cisa.gov/topics/cybersecurity-best...       0.044325\n",
      "4  https://www.qnbtrust.bank/Resources/Learning-C...       0.034733\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "# Initialize the scaler\n",
    "scaler = MinMaxScaler()\n",
    "\n",
    "# Scale the features to be between 0 and 1\n",
    "df['scaled_readability'] = scaler.fit_transform(df[['readability_score']])\n",
    "df['scaled_word_count'] = scaler.fit_transform(df[['word_count']])\n",
    "\n",
    "# Create a composite score. We'll give readability a 60% weight and word count a 40% weight.\n",
    "df['quality_score'] = (0.6 * df['scaled_readability']) + (0.4 * df['scaled_word_count'])\n",
    "\n",
    "# Display the new scores\n",
    "print(\"--- Composite Quality Scores ---\")\n",
    "print(df[['url', 'quality_score']].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e35fb558",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Distribution of Quality Labels ---\n",
      "is_high_quality\n",
      "0    41\n",
      "1    40\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Calculate the median (the middle value) of your quality scores\n",
    "median_quality_score = df['quality_score'].median()\n",
    "\n",
    "# Create the binary label: 1 if the score is above the median, 0 otherwise\n",
    "df['is_high_quality'] = (df['quality_score'] > median_quality_score).astype(int)\n",
    "\n",
    "# Check the distribution of your new label to see how many high/low quality articles you have\n",
    "print(\"\\n--- Distribution of Quality Labels ---\")\n",
    "print(df['is_high_quality'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e3ffd386",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data shape: (64, 4)\n",
      "Testing data shape: (17, 4)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "# Define the feature columns you want the model to learn from\n",
    "feature_columns = ['word_count', 'char_count', 'avg_word_length', 'readability_score']\n",
    "\n",
    "# Create your feature matrix (X) and target vector (y)\n",
    "X = df[feature_columns]\n",
    "y = df['is_high_quality']\n",
    "\n",
    "# Split the data into a training set (80%) and a testing set (20%)\n",
    "# The test set is held back to evaluate the model's performance on unseen data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
    "\n",
    "print(f\"Training data shape: {X_train.shape}\")\n",
    "print(f\"Testing data shape: {X_test.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "4d086c3a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model training complete!\n"
     ]
    }
   ],
   "source": [
    "# Initialize the Logistic Regression model\n",
    "model = LogisticRegression(max_iter=1000, random_state=42)\n",
    "\n",
    "# Train the model on the training data\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "print(\"Model training complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a69c59c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Accuracy: 0.94\n",
      "\n",
      "--- Classification Report ---\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.90      1.00      0.95         9\n",
      "           1       1.00      0.88      0.93         8\n",
      "\n",
      "    accuracy                           0.94        17\n",
      "   macro avg       0.95      0.94      0.94        17\n",
      "weighted avg       0.95      0.94      0.94        17\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Make predictions on the test set\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Calculate the model's accuracy (how many predictions were correct)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"Model Accuracy: {accuracy:.2f}\")\n",
    "\n",
    "# Print a detailed classification report showing precision, recall, and f1-score\n",
    "print(\"\\n--- Classification Report ---\")\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "296ca8a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved to content_quality_model.joblib\n",
      "Vectorizer saved to tfidf_vectorizer.joblib\n"
     ]
    }
   ],
   "source": [
    "import joblib\n",
    "\n",
    "# Define filenames for your model and vectorizer\n",
    "model_filename = 'content_quality_model.joblib'\n",
    "vectorizer_filename = 'tfidf_vectorizer.joblib'\n",
    "\n",
    "# Save the trained model\n",
    "joblib.dump(model, model_filename)\n",
    "\n",
    "# Save the TF-IDF vectorizer\n",
    "joblib.dump(tfidf_vectorizer, vectorizer_filename)\n",
    "\n",
    "print(f\"Model saved to {model_filename}\")\n",
    "print(f\"Vectorizer saved to {vectorizer_filename}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "268b0efc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
